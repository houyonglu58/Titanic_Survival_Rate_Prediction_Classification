{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Titanic Project Example Walk Through \nIn this notebook, I hope to show how a data scientist would go about working through a problem. The goal is to correctly predict if someone survived the Titanic shipwreck. I thought it would be fun to see how well I could do in this compention without deep learning. \n\n*The accompanying video is located here:* https://www.youtube.com/watch?v=I3FBJdiExcg\n\n**Best results : 79.425 % accuracy (Top 12%)**\n\n## Overview \n### 1) Understand the shape of the data (Histograms, box plots, etc.)\n\n### 2) Data Cleaning \n\n### 3) Data Exploration\n\n### 4) Feature Engineering \n\n### 5) Data Preprocessing for Model\n\n### 6) Basic Model Building \n\n### 7) Model Tuning \n\n### 8) Ensemble Modle Building \n\n### 9) Results "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns \nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):# os.walk() returns root, dirs and files\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n# dirname(root) is the directory of the current folder itself.当前文件夹‘/kaggle/input’的路径\n# _(dirs) are the sub folders inside the current folder. 该文件夹'kaggle/input'的子目录名list\n# filenames (files) are the file names inside the current. 该文件夹‘kaggle/input’的文件名list\n\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we import the data. For this analysis, we will be exclusively working with the Training set. We will be validating based on data from the training set as well. For our final submissions, we will make predictions based on the test set. "},{"metadata":{"trusted":true},"cell_type":"code","source":"training = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest = pd.read_csv('/kaggle/input/titanic/test.csv')\n\ntraining['train_test'] = 1 # to isolate data from training to testing\ntest['train_test'] = 0\ntest['Survived'] = np.NaN\nall_data = pd.concat([training,test])\n\n%matplotlib inline\nall_data.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Project Planning\nWhen starting any project, I like to outline the steps that I plan to take. Below is the rough outline that I created for this project using commented cells. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Understand nature of the data .info() .describe()\n# Histograms and boxplots \n# Value counts \n# Missing data \n# Correlation between the metrics \n# Explore interesting themes \n    # Wealthy survive? \n    # By location \n    # Age scatterplot with ticket price \n    # Young and wealthy Variable? \n    # Total spent? \n# Feature engineering \n# preprocess data together or use a transformer? \n    # use label for train and test   \n# Scaling?\n\n# Model Baseline \n# Model comparison with CV ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Light Data Exploration\n### 1) For numeric data \n* Made histograms to understand distributions (other options: boxplot, violinplot)\n* Corrplot \n* Pivot table comparing survival rate across numeric variables \n\n\n### 2) For Categorical Data \n* Made bar charts to understand balance of classes (other options, countplot/histogram )\n* Made pivot tables to understand relationship with survival "},{"metadata":{"trusted":true},"cell_type":"code","source":"#quick look at our data types & null counts \ntraining.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# to better understand the numeric data, we want to use the .describe() method. This gives us an understanding of the central tendencies of the data \ntraining.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#quick way to separate numeric columns\ntraining.describe().columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# look at numeric and categorical values separately \ndf_num = training[['Age','SibSp','Parch','Fare']] # histogram/box plot to find distribution\ndf_cat = training[['Survived','Pclass','Sex','Ticket','Cabin','Embarked']] #count plot/value count to find frequency","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#distributions for all numeric variables \nfor i in df_num.columns:\n    plt.hist(df_num[i])\n    plt.title(i)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Perhaps we should take the non-normal distributions and consider normalizing them? Apparently, except for age, the rest three numerical variables do not display a normal distribution. "},{"metadata":{},"cell_type":"markdown","source":"\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_num.corr())\nsns.heatmap(df_num.corr())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When running linear regression, we want to avoid adopting variables that have multicolinearity which means there exist strong correlations between one another. \n\nHere, both slipsp and partch seem to have a slight negative correation with age, while parch and sibsp appear to have a moderately postive correlation with each other."},{"metadata":{"trusted":true},"cell_type":"code","source":"# compare survival rate across Age, SibSp, Parch, and Fare \npd.pivot_table(training, index = 'Survived', values = ['Age','SibSp','Parch','Fare'])\n\n# 等价pivot table和group by\n# pd.pivot_table(df,index=[字段1],values=[字段2],aggfunc=[函数],fill_value=0)默认mean函数\n# df.groupby([字段1])[字段2].agg(函数).fillna(0) \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"My personal attempt to achieve the same effect generated by pivot_table() with \"groupby().agg()\" function.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntraining.groupby([\"Survived\"])[['Age','SibSp','Parch','Fare']].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# use bar plot/countplot to measure the count differences among categorical variables\nfor i in df_cat.columns:\n    sns.barplot(df_cat[i].value_counts().index,df_cat[i].value_counts()).set_title(i)\n    plt.show()\\\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cabin and ticket graphs are very messy. This is an area where we may want to do some feature engineering! "},{"metadata":{},"cell_type":"markdown","source":"My personal attempt to achieve the same effect generated by sns.barplot(x.value_counts().index, x.value_counts()) with \"sns.countplot()\" function."},{"metadata":{"trusted":true},"cell_type":"code","source":"# try to integrate all four graphs in one figure\nfig, axss = plt.subplots(2,3,figsize=[20,10])\nsns.countplot(x=df_cat.columns[0], data=df_cat, ax=axss[0][0])\nsns.countplot(x=df_cat.columns[1], data=df_cat, ax=axss[0][1])\nsns.countplot(x=df_cat.columns[2], data=df_cat, ax=axss[0][2])\nsns.countplot(x=df_cat.columns[3], data=df_cat, ax=axss[1][0])\nsns.countplot(x=df_cat.columns[4], data=df_cat, ax=axss[1][1])\nsns.countplot(x=df_cat.columns[5], data=df_cat, ax=axss[1][2])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Comparing survival and each of these categorical variables \nprint(pd.pivot_table(training, index = 'Survived', columns = 'Pclass', values = 'Ticket' ,aggfunc ='count'))\nprint()\nprint(pd.pivot_table(training, index = 'Survived', columns = 'Sex', values = 'Ticket' ,aggfunc ='count'))\nprint()\nprint(pd.pivot_table(training, index = 'Survived', columns = 'Embarked', values = 'Ticket' ,aggfunc ='count'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering \n### 1) Cabin - Simplify cabins (evaluated if cabin letter (cabin_adv) or the purchase of tickets across multiple cabins (cabin_multiple) impacted survival)\n\n### 2) Tickets - Do different ticket types impact survival rates?\n\n### 3) Does a person's title relate to survival rates? "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cat.Cabin\n\n# part 1\n# find a potential feature \"cabin_multiple\" and create a new column \"cabin_multiple\"\ntraining['cabin_multiple'] = training.Cabin.apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))\n\n# after looking at this, we may want to look at cabin by letter or by number. Let's create some categories for this \n# letters \n# multiple letters \ntraining['cabin_multiple'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# compare survival rate by cabin_multiple\npd.pivot_table(training, index = 'Survived', columns = 'cabin_multiple', values = 'Ticket' ,aggfunc ='count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creates categories based on the cabin letter (n stands for null)\n#in this case we will treat null values like it's own category\n\n# part 2\n# find a potential feature \"cabin_adv\" and create a new column \"cabin_adv\"\ntraining['cabin_adv'] = training.Cabin.apply(lambda x: str(x)[0])\n\n# n represents \"null\" and it seems like there are lots of null for cabin values\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#comparing surivial rate by cabin\nprint(training.cabin_adv.value_counts())\npd.pivot_table(training,index='Survived',columns='cabin_adv', values = 'Name', aggfunc='count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clearly, people who don't have specific cabin names have much higher death rate. Those with specific "},{"metadata":{"trusted":true},"cell_type":"code","source":"# find a potential feature \"ticket types\" and create a new pair of column \"numeric_ticket\" and \"ticket_letters\"\n\n#understand ticket values better \n#numeric vs non numeric \ntraining['numeric_ticket'] = training.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\ntraining['ticket_letters'] = training.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.','').replace('/','').lower() if len(x.split(' ')[:-1]) >0 else 0)\n\n# here the last component of ticket, as seperated by \" \", is always digit. Thus we are excluding it. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training['numeric_ticket'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training['ticket_letters'].head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets us view all rows in dataframe through scrolling. This is for convenience \npd.set_option(\"max_rows\", None)\ntraining['ticket_letters'].value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#difference in numeric vs non-numeric tickets in survival rate \npd.pivot_table(training,index='Survived',columns='numeric_ticket', values = 'Ticket', aggfunc='count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#survival rate across different tyicket types \npd.pivot_table(training,index='Survived',columns='ticket_letters', values = 'Ticket', aggfunc='count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#feature engineering on person's title \ntraining.Name.head(50)\ntraining['name_title'] = training.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())\n#mr., ms., master. etc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training['name_title'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preprocessing for Model \n### 1) Drop null values from Embarked (only 2) \n\n### 2) Include only relevant variables (Since we have limited data, I wanted to exclude things like name and passanger ID so that we could have a reasonable number of features for our models to deal with) \nVariables:  'Pclass', 'Sex','Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'cabin_adv', 'cabin_multiple', 'numeric_ticket', 'name_title'\n\n### 3) Do categorical transforms on all data. Usually we would use a transformer, but with this approach we can ensure that our traning and test data have the same colums. We also may be able to infer something about the shape of the test data through this method. I will stress, this is generally not recommend outside of a competition (use onehot encoder，or ordinal encoder). \n\n### 4) Impute data with mean for fare and age (Should also experiment with median) \n\n### 5) Normalized fare using logarithm to give more semblance of a normal distribution (log normalization > transform into 0 - 1 range)\n\n### 6) Scaled data 0-1 with standard scaler (standardization > turn to normal distribution)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#create all categorical variables that we did above for both training and test sets \nall_data['cabin_multiple'] = all_data.Cabin.apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))\nall_data['cabin_adv'] = all_data.Cabin.apply(lambda x: str(x)[0])\nall_data['numeric_ticket'] = all_data.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\nall_data['ticket_letters'] = all_data.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.','').replace('/','').lower() if len(x.split(' ')[:-1]) >0 else 0)\nall_data['name_title'] = all_data.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())\n\n#impute nulls for continuous data \n#all_data.Age = all_data.Age.fillna(training.Age.mean())\nall_data.Age = all_data.Age.fillna(training.Age.median())\n#all_data.Fare = all_data.Fare.fillna(training.Fare.mean())\nall_data.Fare = all_data.Fare.fillna(training.Fare.median())\n\n#drop null 'embarked' rows. Only 2 instances of this in training and 0 in test \nall_data.dropna(subset=['Embarked'],inplace = True)\n\n#tried log normalization of sibsp (not used)\nall_data['norm_sibsp'] = np.log(all_data.SibSp+1)\nall_data['norm_sibsp'].hist()\n \n# log norm of fare (used)\nall_data['norm_fare'] = np.log(all_data.Fare+1)\nall_data['norm_fare'].hist()\n\n# converted fare to category for pd.get_dummies()\nall_data.Pclass = all_data.Pclass.astype(str)\n\n#created dummy variables from categories (also can use OneHotEncoder from sklearn.preprocessing)\nall_dummies = pd.get_dummies(all_data[['Pclass','Sex','Age','SibSp','Parch','norm_fare','Embarked','cabin_adv','cabin_multiple','numeric_ticket','name_title','train_test']])\n\nall_dummies.head(20)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split to train test again\nX_train = all_dummies[all_dummies.train_test == 1].drop(['train_test'], axis =1)\nX_test = all_dummies[all_dummies.train_test == 0].drop(['train_test'], axis =1)\n\n\ny_train = all_data[all_data.train_test==1].Survived # get Survived column from training data only\ny_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scale data to normal distribution (zero mean, unit varience) to avoid extreme values with huge varience dominating parameters estimation of function \nfrom sklearn.preprocessing import StandardScaler\nscale = StandardScaler()\nall_dummies_scaled = all_dummies.copy()\nall_dummies_scaled[['Age','SibSp','Parch','norm_fare']]= scale.fit_transform(all_dummies_scaled[['Age','SibSp','Parch','norm_fare']])\nall_dummies_scaled[['Age','SibSp','Parch','norm_fare']].head(20)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_scaled = all_dummies_scaled[all_dummies_scaled.train_test == 1].drop(['train_test'], axis =1)\nX_test_scaled = all_dummies_scaled[all_dummies_scaled.train_test == 0].drop(['train_test'], axis =1)\n\ny_train = all_data[all_data.train_test==1].Survived","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Building (Baseline Validation Performance)\nBefore going further, I like to see how various different models perform with default parameters. I tried the following models using 5 fold cross validation to get a baseline. With a validation set basline, we can see how much tuning improves each of the models. Just because a model has a high basline on this validation set doesn't mean that it will actually do better on the eventual test set. \n\n- Naive Bayes (72.6%) \n- Logistic Regression (82.1%) \n- Decision Tree (77.6%) \n- K Nearest Neighbor (80.5%) \n- Random Forest (80.6%) \n- **Support Vector Classifier (83.2%)**\n- Xtreme Gradient Boosting (81.8%)\n- Soft Voting Classifier - All Models (82.8%)\n\nThe above mentioned classification models all have mean accuracy as their default score. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#I usually use Naive Bayes as a baseline for my classification tasks \ngnb = GaussianNB() \ncv = cross_val_score(gnb,X_train_scaled,y_train,cv=5) # by default the score is estimator's default score\nprint(cv)\nprint(cv.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cross validation accelerates the splitting of training and testing data for multiple times. Value of cv decides the times of training as well.\n\nOtherwise, for each training, it requires train_test_split(), model.fit(training), model.score(testing) to generate the accuracy of one-time training. "},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression(max_iter = 2000)\ncv = cross_val_score(lr,X_train,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Max_iter regulates the maximum iteration frequency for gradient descent to get the minimum loss function."},{"metadata":{"trusted":true},"cell_type":"code","source":"dt = tree.DecisionTreeClassifier(random_state = 1)\ncv = cross_val_score(dt,X_train,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Setting random_state to a fixed interger ensures a deterministic splitting behaviours during fitting. "},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier()\ncv = cross_val_score(knn,X_train,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(random_state = 1)\ncv = cross_val_score(rf,X_train,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(random_state = 1)\ncv = cross_val_score(rf,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svc = SVC(probability = True)\ncv = cross_val_score(svc,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SVM enables classification of data which are linearly seperable, and non-linearly seperate with the kernel trick setting. SVM algorithm finds the hyperplane that maximizes the margin from both tags. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\nxgb = XGBClassifier(random_state =1)\ncv = cross_val_score(xgb,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Voting classifier takes all of the inputs and averages the results. For a \"hard\" voting classifier each classifier gets 1 vote \"yes\" or \"no\" and the result is just a popular vote. For this, you generally want odd numbers\n#A \"soft\" classifier averages the confidence of each of the models. If a the average confidence is > 50% that it is a 1 it will be counted as such\nfrom sklearn.ensemble import VotingClassifier\nvoting_clf = VotingClassifier(estimators = [('lr',lr),('knn',knn),('rf',rf),('gnb',gnb),('svc',svc),('xgb',xgb)], voting = 'soft') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = cross_val_score(voting_clf,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"voting_clf.fit(X_train_scaled,y_train)\ny_hat_base_vc = voting_clf.predict(X_test_scaled).astype(int)\nbasic_submission = {'PassengerId': test.PassengerId, 'Survived': y_hat_base_vc}\nbase_submission = pd.DataFrame(data=basic_submission)\nbase_submission.to_csv('base_submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Tuned Performance \nAfter getting the baselines, let's see if we can improve on the indivdual model results!I mainly used grid search to tune the models. I also used Randomized Search for the Random Forest and XG boosted model to simplify testing time. \n\n|Model|Baseline|Tuned Performance|\n|-----|--------|-----------------|\n|Naive Bayes| 72.6%| NA|\n|Logistic Regression| 82.1%| 82.6%|\n|Decision Tree| 77.6%| NA|\n|K Nearest Neighbor| 80.5%|83.0%|\n|Random Forest| 80.6%| 83.6|\n|Support Vector Classifier| 83.2%| 83.2%|\n|Xtreme Gradient Boosting| 81.8%| 85.3%|\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV \nfrom sklearn.model_selection import RandomizedSearchCV ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#simple performance reporting function\ndef clf_performance(classifier, model_name):\n    print(model_name)\n    print('Best Score: ' + str(classifier.best_score_))\n    print('Best Parameters: ' + str(classifier.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# find names and current values for all parameters for a given estimator\nlr = LogisticRegression()\nlr.get_params()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression()\nparam_grid = {'max_iter' : [2000],\n              'penalty' : ['l1', 'l2'],\n              'C' : np.logspace(-4, 4, 20),\n              'solver' : ['liblinear']}\n# C is the inverse of regularization strength.\n# np.logspace(start, end, num) generates numbers spaced evently on a log scale (base=10 by default)\n# solver defines the algorithm to use in optimization of loss function. \n# liblinear is the only one suitable for both L1 and L2 regularizaton. The others (lbfgs, sag, newton-cg) requires either first derivative and second derivative. \n\n\nclf_lr = GridSearchCV(lr, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_lr = clf_lr.fit(X_train_scaled,y_train)\nclf_performance(best_clf_lr,'Logistic Regression')\n# n_jobs=-1 sets all CPUs to be used concurrently. \n# verbose=True display more massages about cross validation and fitting.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier()\nparam_grid = {'n_neighbors' : [3,5,7,9],\n              'weights' : ['uniform', 'distance'],\n              'algorithm' : ['auto', 'ball_tree','kd_tree'],\n              'p' : [1,2]}\n# weights (uniform: points equally weighed; distance:points weighed by the inverse of their distance > the closer, the greater influencce.)\n# algorithm defines the algorithms used to fastly compute the nearest neighbors (ball tree, kd tree are both methods for fast generalizatio of N-point. auto will decide the most appropriate algorithm based on the values passed to the fit.)\n# p=1 using manhattan distance, p=2 using euclidean distance\n\nclf_knn = GridSearchCV(knn, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_knn = clf_knn.fit(X_train_scaled,y_train)\nclf_performance(best_clf_knn,'KNN')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svc = SVC(probability = True)\nparam_grid = tuned_parameters = [{'kernel': ['rbf'], 'gamma': [.1,.5,1,2,5,10],\n                                  'C': [.1, 1, 10, 100, 1000]},\n                                 {'kernel': ['linear'], 'C': [.1, 1, 10, 100, 1000]},\n                                 {'kernel': ['poly'], 'degree' : [2,3,4,5], 'C': [.1, 1, 10, 100, 1000]}]\n\n# C (aka. penalty parameter) defines the inverse of regularization strength. \n# kernal: (linear for linear hyperplane; poly and rbf are non linear hyperplanes)\n# gamma is a parameter for non linear hyperplanes, the higher the more exactly fitted for model (overfitting) in the training data set.\n# \n\nclf_svc = GridSearchCV(svc, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_svc = clf_svc.fit(X_train_scaled,y_train)\nclf_performance(best_clf_svc,'SVC')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Because the total feature space is so large, I used a randomized search to narrow down the paramters for the model. I took the best model from this and did a more granular search \n\"\"\"\nrf = RandomForestClassifier(random_state = 1)\nparam_grid =  {'n_estimators': [100,500,1000], \n                                  'bootstrap': [True,False],\n                                  'max_depth': [3,5,10,20,50,75,100,None],\n                                  'max_features': ['auto','sqrt'],\n                                  'min_samples_leaf': [1,2,4,10],\n                                  'min_samples_split': [2,5,10]}\n                                  \nclf_rf_rnd = RandomizedSearchCV(rf, param_distributions = param_grid, n_iter = 100, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_rf_rnd = clf_rf_rnd.fit(X_train_scaled,y_train)\nclf_performance(best_clf_rf_rnd,'Random Forest')\n\nGridSearch experiements all permutation of parameters. RandomizedSearch conducts randome sampling of parameter settings to find the best ones.\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(random_state = 1)\nparam_grid =  {'n_estimators': [400,450,500,550],\n               'criterion':['gini','entropy'],\n                                  'bootstrap': [True],\n                                  'max_depth': [15, 20, 25],\n                                  'max_features': ['auto','sqrt', 10],\n                                  'min_samples_leaf': [2,3],\n                                  'min_samples_split': [2,3]}\n                                  \nclf_rf = GridSearchCV(rf, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_rf = clf_rf.fit(X_train_scaled,y_train)\nclf_performance(best_clf_rf,'Random Forest')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_rf = best_clf_rf.best_estimator_.fit(X_train_scaled,y_train)\nfeat_importances = pd.Series(best_rf.feature_importances_, index=X_train_scaled.columns)\nfeat_importances.nlargest(20).plot(kind='barh')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Feature Importance for Tree Models (Random Forest and Gradient Boosting)\n\nRandom Forest\n1. Gini Importance/Mean Decrease in Impurity \n\nMeasure Gini importance of each node and select nodes (features) with higher Gini importance.\n\n* Gini index: measure data purity of each node, formula = 1-Epi^2 (pi = probability of each feature), (the lower the index value, the higher the purity) \n* Gini decrease: Gini index of each node subtract all of its subnodes' Gini index (weighted)\n* Gini importance: sum of Gini decrease (weighted) of all the same nodes across the forest (trees)\n\nFeatures are selected based on Gini Importance (the measurement of purity of features)\n\nDisadvantage: \n1) It is positively biased against features with more option values\n2) For the features with multicolinearity, each of these could be an important feature. However, once one is selected, the others' importance will decrease immediately, making it more difficult to be selected. It is likely to cause misunderstanding that the one selected is very important while the others are not!!\n\n\n2. Permutation Importance/Mean Decrease in Accuracy (theoretically applicable to all tree-bsaed models)\n\nRandomly permutate the feature order to measure the impact such alternation can bring to the original model performance (i.e. comparison of accuracy before and after)\n\n1) step 1: train a rf model and get accuracy0\n2) step 2: randomly permutate feature A and get accuracy 1\n3) step 3: (accuracy 0 - accuracy 1)/accuracy 0 to get feature A's importance. \n\nThe higher the importance Feature X has, the bigger the impact there is after the random permutation. \n\n\n3. Boruta (theoretically applicable to all tree-based models)\n\nMeasure feature importance based on iterative comparison of original features and shadow features, stop when all original features are compared or a predefined rule has been reached. \n\n1) Create shadow features: add randome interference to the origianl features, then random sampling from extended features to get n number of shadow features.\n2) Compare each original feature with the one having the highest importance in shadow feature set to decide whether this original feature is important or not.\n\nAdvantage: \n1) Take all features into consideration, regardless of how strong the relevance it has with determining features\n2) Enable feature selection through iterations to minimize the error caused by random forest. \n\nGradient Boosting:\n1. Split-based and Gain-based Measures\n\nSplit and gain are both feature attribution methods used to measure the feature importance. Sometimes the results from different methods are inconsistent, SHAP will be used to maintian this consistence.\n\nSplit: The number of times when a feature appears as node in the whole tree model. \n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"xgb = XGBClassifier(random_state = 1)\n\nparam_grid = {\n    'n_estimators': [20, 50, 100, 250, 500,1000],\n    'colsample_bytree': [0.2, 0.5, 0.7, 0.8, 1],\n    'max_depth': [2, 5, 10, 15, 20, 25, None],\n    'reg_alpha': [0, 0.5, 1],\n    'reg_lambda': [1, 1.5, 2],\n    'subsample': [0.5,0.6,0.7, 0.8, 0.9],\n    'learning_rate':[.01,0.1,0.2,0.3,0.5, 0.7, 0.9],\n    'gamma':[0,.01,.1,1,10,100],\n    'min_child_weight':[0,.01,0.1,1,10,100],\n    'sampling_method': ['uniform', 'gradient_based']\n}\n\n#clf_xgb = GridSearchCV(xgb, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\n#best_clf_xgb = clf_xgb.fit(X_train_scaled,y_train)\n#clf_performance(best_clf_xgb,'XGB')\nclf_xgb_rnd = RandomizedSearchCV(xgb, param_distributions = param_grid, n_iter = 1000, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_xgb_rnd = clf_xgb_rnd.fit(X_train_scaled,y_train)\nclf_performance(best_clf_xgb_rnd,'XGB')\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**AdaBoost:**\n\nFocusing on wrongly classified samples from the previous classifier by giving it more weights in the next classifier, aiming to reduce the wrong classification results.\n\nAssign each classifer a weight and integrate all classifiers (weighted) together to get the final answer.\n\n**GBDT (ensembling learning):** \n\nFocusing on residual, the difference between previous model's result from the real answer, by establishing a new classifier on the fastest direction of the residual decreasing (gradient descent), aiming to reduce the value of residuel.\n\nSum up the results from all classifiers to get the final answer.\n\n**XGBoost (ensembleing learning) :**\n\nSimilar to GBDT. Focusing on residual, the difference between previous model's result from the real answer, by establishing a new classifier on the minimizing the user-definable loss function (regularization enabled), aiming to reduce the value of residuel.\n\nSum up the results from all classifiers to get the final answer.\n\nGBDT and XGBoost have similar strategies but differnt methods to minimize residual 残差\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb = XGBClassifier(random_state = 1)\n\nparam_grid = {\n    'n_estimators': [450,500,550],\n    'colsample_bytree': [0.75,0.8,0.85],\n    'max_depth': [None],\n    'reg_alpha': [1],\n    'reg_lambda': [2, 5, 10],\n    'subsample': [0.55, 0.6, .65],\n    'learning_rate':[0.5],\n    'gamma':[.5,1,2],\n    'min_child_weight':[0.01],\n    'sampling_method': ['uniform']\n}\n\n# colsample_bytree: the ratio of subsample of features for every tree constructed\n# reg_alpha: weight for L1 regularization. \n# reg_lambda: weight for L2 regularization.\n# sub_sample: the ratio of training data for every boosting iteration.\n# learning_rate: step size shrinkage for gradient descrent.\n# gamma: minimum loss reduction before making further partition on a leaf node.\n# min_child_weight: minimum sum of instance rate for a child before it makes further partition.\n# sampling_method: ways to sample data from training instances (uniform: equal probability)\n\n\nclf_xgb = GridSearchCV(xgb, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_xgb = clf_xgb.fit(X_train_scaled,y_train)\nclf_performance(best_clf_xgb,'XGB')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_hat_xgb = best_clf_xgb.best_estimator_.predict(X_test_scaled).astype(int)\nxgb_submission = {'PassengerId': test.PassengerId, 'Survived': y_hat_xgb}\nsubmission_xgb = pd.DataFrame(data=xgb_submission)\nsubmission_xgb.to_csv('xgb_submission3.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Additional Ensemble Approaches \n\nSoft voting/Majority Rule classifir for unfitted estimators.\n\n1) Experimented with a hard voting classifier of three estimators (KNN, SVM, RF) (81.6%)\n\n2) **Experimented with a soft voting classifier of three estimators (KNN, SVM, RF) (82.3%) (Best Performance)**\n\n3) Experimented with soft voting on all estimators performing better than 80% except xgb (KNN, RF, LR, SVC) (82.9%)\n\n4) Experimented with soft voting on all estimators including XGB (KNN, SVM, RF, LR, XGB) (83.5%)\n\n**Hard Voting:** determine final results using predicted class labels on majority rule basis.\n\n**Soft Voting:** determine final results using argmax of the sums of the predicted probabilties (recommended for ensembling of calibrated ensembler). The label with the highest average argmax sum of total probabilities will be considered as the final result. "},{"metadata":{"trusted":true},"cell_type":"code","source":"best_lr = best_clf_lr.best_estimator_\nbest_knn = best_clf_knn.best_estimator_\nbest_svc = best_clf_svc.best_estimator_\nbest_rf = best_clf_rf.best_estimator_\nbest_xgb = best_clf_xgb.best_estimator_\n\nvoting_clf_hard = VotingClassifier(estimators = [('knn',best_knn),('rf',best_rf),('svc',best_svc)], voting = 'hard') \nvoting_clf_soft = VotingClassifier(estimators = [('knn',best_knn),('rf',best_rf),('svc',best_svc)], voting = 'soft') \nvoting_clf_all = VotingClassifier(estimators = [('knn',best_knn),('rf',best_rf),('svc',best_svc), ('lr', best_lr)], voting = 'soft') \nvoting_clf_xgb = VotingClassifier(estimators = [('knn',best_knn),('rf',best_rf),('svc',best_svc), ('xgb', best_xgb),('lr', best_lr)], voting = 'soft')\n\nprint('voting_clf_hard :',cross_val_score(voting_clf_hard,X_train,y_train,cv=5))\nprint('voting_clf_hard mean :',cross_val_score(voting_clf_hard,X_train,y_train,cv=5).mean())\n\nprint('voting_clf_soft :',cross_val_score(voting_clf_soft,X_train,y_train,cv=5))\nprint('voting_clf_soft mean :',cross_val_score(voting_clf_soft,X_train,y_train,cv=5).mean())\n\nprint('voting_clf_all :',cross_val_score(voting_clf_all,X_train,y_train,cv=5))\nprint('voting_clf_all mean :',cross_val_score(voting_clf_all,X_train,y_train,cv=5).mean())\n\nprint('voting_clf_xgb :',cross_val_score(voting_clf_xgb,X_train,y_train,cv=5))\nprint('voting_clf_xgb mean :',cross_val_score(voting_clf_xgb,X_train,y_train,cv=5).mean())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Voting Classifier can assign weights for each classifier, which will affect the soft voting argmax of the sums of the predicted probabilities. \n\nVoting Classifier can also be implemeted with GridSearchCV to fine-tune the parameters of the estimotors embedded in the voting classifier. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#in a soft voting classifier you can weight some models more than others. I used a grid search to explore different weightings\n#no new results here\nparams = {'weights' : [[1,1,1],[1,2,1],[1,1,2],[2,1,1],[2,2,1],[1,2,2],[2,1,2]]}\n\nvote_weight = GridSearchCV(voting_clf_soft, param_grid = params, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_weight = vote_weight.fit(X_train_scaled,y_train)\nclf_performance(best_clf_weight,'VC Weights')\nvoting_clf_sub = best_clf_weight.best_estimator_.predict(X_test_scaled)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Make Predictions \nvoting_clf_hard.fit(X_train_scaled, y_train)\nvoting_clf_soft.fit(X_train_scaled, y_train)\nvoting_clf_all.fit(X_train_scaled, y_train)\nvoting_clf_xgb.fit(X_train_scaled, y_train)\n\nbest_rf.fit(X_train_scaled, y_train)\ny_hat_vc_hard = voting_clf_hard.predict(X_test_scaled).astype(int)\ny_hat_rf = best_rf.predict(X_test_scaled).astype(int)\ny_hat_vc_soft =  voting_clf_soft.predict(X_test_scaled).astype(int)\ny_hat_vc_all = voting_clf_all.predict(X_test_scaled).astype(int)\ny_hat_vc_xgb = voting_clf_xgb.predict(X_test_scaled).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#convert output to dataframe \nfinal_data = {'PassengerId': test.PassengerId, 'Survived': y_hat_rf}\nsubmission = pd.DataFrame(data=final_data)\n\nfinal_data_2 = {'PassengerId': test.PassengerId, 'Survived': y_hat_vc_hard}\nsubmission_2 = pd.DataFrame(data=final_data_2)\n\nfinal_data_3 = {'PassengerId': test.PassengerId, 'Survived': y_hat_vc_soft}\nsubmission_3 = pd.DataFrame(data=final_data_3)\n\nfinal_data_4 = {'PassengerId': test.PassengerId, 'Survived': y_hat_vc_all}\nsubmission_4 = pd.DataFrame(data=final_data_4)\n\nfinal_data_5 = {'PassengerId': test.PassengerId, 'Survived': y_hat_vc_xgb}\nsubmission_5 = pd.DataFrame(data=final_data_5)\n\nfinal_data_comp = {'PassengerId': test.PassengerId, 'Survived_vc_hard': y_hat_vc_hard, 'Survived_rf': y_hat_rf, 'Survived_vc_soft' : y_hat_vc_soft, 'Survived_vc_all' : y_hat_vc_all,  'Survived_vc_xgb' : y_hat_vc_xgb}\ncomparison = pd.DataFrame(data=final_data_comp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#track differences between outputs \ncomparison['difference_rf_vc_hard'] = comparison.apply(lambda x: 1 if x.Survived_vc_hard != x.Survived_rf else 0, axis =1)\ncomparison['difference_soft_hard'] = comparison.apply(lambda x: 1 if x.Survived_vc_hard != x.Survived_vc_soft else 0, axis =1)\ncomparison['difference_hard_all'] = comparison.apply(lambda x: 1 if x.Survived_vc_all != x.Survived_vc_hard else 0, axis =1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comparison.difference_hard_all.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#prepare submission files \nsubmission.to_csv('submission_rf.csv', index =False)\nsubmission_2.to_csv('submission_vc_hard.csv',index=False)\nsubmission_3.to_csv('submission_vc_soft.csv', index=False)\nsubmission_4.to_csv('submission_vc_all.csv', index=False)\nsubmission_5.to_csv('submission_vc_xgb2.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}